

import os
import sys
import torch
import torch.nn.functional as F
import requests
import random
import numpy as np
import re
from urllib.parse import urlparse # URL íŒŒì‹±ì„ ìœ„í•´ ì¶”ê°€

from pytorch_pretrained_bert import BertTokenizer
from lime.lime_text import LimeTextExplainer


project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)


from config import (
    PAD_SIZE, DEVICE, CLASS_LABELS, IMPORTANT_HEADERS,
    REQUEST_TIMEOUT_SECONDS, LIME_NUM_FEATURES, LIME_NUM_SAMPLES,
    TRUSTED_DOMAINS_FOR_EXPLANATION # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ ëª©ë¡
)

# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ (core)
current_dir = os.path.dirname(os.path.abspath(__file__))
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ (urlbert2)
project_root = os.path.abspath(os.path.join(current_dir, '..'))

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²€ìƒ‰ ê²½ë¡œì— ì¶”ê°€
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# --- HTTP í—¤ë” ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜ ---
def get_header_info(url: str) -> str:
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/114 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 Version/14.0.3 Safari/605.1.15",
        "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 Version/14.0 Mobile/15E148 Safari/604.1"
    ]
    headers = {
        "User-Agent": random.choice(user_agents),
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": "https://www.google.com/"
    }

    try:
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT_SECONDS, allow_redirects=True)
        resp_headers = response.headers
        
        important = {
            k: resp_headers.get(k, "") for k in IMPORTANT_HEADERS
        }
        header_str = ", ".join(f"{k}: {v}" for k, v in important.items() if v)
        return header_str if header_str else "NOHEADER"
    except requests.exceptions.RequestException:
        return "NOHEADER"
    except Exception:
        return "NOHEADER"

# --- ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
def preprocess_url_for_inference(url: str, header_info: str, tokenizer: BertTokenizer, pad_size: int = PAD_SIZE):
    text = f"{url} [SEP] {header_info}"
    tokenized_text = tokenizer.tokenize(text)
    
    tokens = ["[CLS]"] + tokenized_text + ["[SEP]"]
    ids = tokenizer.convert_tokens_to_ids(tokens)
    types = [0] * (len(ids))
    masks = [1] * len(ids)

    if len(ids) < pad_size:
        types = types + [1] * (pad_size - len(ids))
        masks = masks + [0] * (pad_size - len(ids))
        ids = ids + [0] * (pad_size - len(ids))
    else:
        types = types[:pad_size]
        masks = masks[:pad_size]
        ids = ids[:pad_size]

    assert len(ids) == len(masks) == len(types) == pad_size

    return (
        torch.tensor([ids], dtype=torch.long).to(DEVICE),
        torch.tensor([types], dtype=torch.long).to(DEVICE),
        torch.tensor([masks], dtype=torch.long).to(DEVICE)
    )

# --- LIME ì„¤ëª…ìë¥¼ ìœ„í•œ ì˜ˆì¸¡ í•¨ìˆ˜ ---
def lime_predictor_fn(texts, model, tokenizer):
    probabilities = []
    for text_input in texts:
        parts = text_input.split(" [SEP] ", 1)
        url_part = parts[0]
        header_part = parts[1] if len(parts) > 1 else "NOHEADER"
        
        input_ids, input_types, input_masks = preprocess_url_for_inference(
            url_part, header_part, tokenizer, PAD_SIZE
        )

        with torch.no_grad():
            outputs = model([input_ids, input_types, input_masks])
            probs = F.softmax(outputs, dim=1).cpu().numpy()
            probabilities.append(probs[0])
            
    return np.array(probabilities)


# --- 1. ëª¨ë¸ ì˜ˆì¸¡ë§Œ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ ---
def predict_url(url: str, model, tokenizer) -> dict:
    header_info = get_header_info(url)
    
    input_ids, input_types, input_masks = preprocess_url_for_inference(
        url, header_info, tokenizer, PAD_SIZE
    )

    with torch.no_grad():
        outputs = model([input_ids, input_types, input_masks])
        probabilities = F.softmax(outputs, dim=1)
        predicted_class_id = torch.argmax(probabilities, dim=1).item()

    predicted_label = CLASS_LABELS[predicted_class_id]
    confidence = probabilities[0][predicted_class_id].item() # 0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ ë°˜í™˜
    
    return {
        "predicted_label": predicted_label,
        "confidence": confidence, # 0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ ë°˜í™˜
        "predicted_class_id": predicted_class_id,
        "header_info": header_info # LIME ì„¤ëª…ì„ ìœ„í•´ í—¤ë” ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜
    }

# --- 2. LIME ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ ---
def explain_prediction_with_lime(url: str, header_info: str, model, tokenizer, predicted_class_id: int) -> dict:
    predicted_label = CLASS_LABELS[predicted_class_id]
    full_text_for_lime = f"{url} [SEP] {header_info}"
    
    explanation_list = []
    reason_summary = f"íŒë‹¨ ê·¼ê±°ë¥¼ ì„¤ëª…í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    # URLì„ í•œ ë²ˆë§Œ íŒŒì‹±í•˜ì—¬ í•¨ìˆ˜ ì „ì²´ì—ì„œ ì‚¬ìš©
    parsed_url = urlparse(url)
    netloc_lower = parsed_url.netloc.lower()
    tld = parsed_url.netloc.split('.')[-1] if '.' in parsed_url.netloc else ''

    try:
        explainer = LimeTextExplainer(class_names=list(CLASS_LABELS.values()))

        explanation = explainer.explain_instance(
            full_text_for_lime,
            classifier_fn=lambda texts: lime_predictor_fn(texts, model, tokenizer),
            labels=[0, 1],
            num_features=LIME_NUM_FEATURES,
            num_samples=LIME_NUM_SAMPLES
        )

        explanation_list = explanation.as_list(label=predicted_class_id)
        
        # --- LIME ì„¤ëª… ìš”ì•½ ë° ìƒì„¸ ì„¤ëª… ë¡œì§ ê°œì„  ---
        def get_understandable_explanation_text_and_meaning(word, weight, current_predicted_label, original_url, parsed_url_info, netloc_lower_info, tld_info):
            word_lower = word.strip().lower() # ì—¬ê¸°ì„œ word_lower ì •ì˜ ë° ì‚¬ìš©
            
            # 1ë‹¨ê³„ í•„í„°ë§: BERT íŠ¹ìˆ˜ í† í° ë° LIME ë…¸ì´ì¦ˆ í•„í„°ë§ ê°•í™”
            # ë‹¨ì¼ íŠ¹ìˆ˜ë¬¸ì, ì§§ì€ ìˆ«ì, í”í•œ ì„¸ì…˜ ì¿ í‚¤ ë“±ì€ LIMEì´ ì˜ë¯¸ ì—†ê²Œ ë½‘ì•„ë‚¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•„í„°ë§
            if word_lower in ["[sep]", "[cls]", "[pad]", "sep", "cls", "pad", "##s", "com", "co", "kr", "net", "org", "io", "ai", "app", "ly", "me", "biz", "info", "name", 
                              "php", "html", "asp", "aspx", "htm", "default", "index", # í”í•œ íŒŒì¼ëª…/í™•ì¥ì
                              "session", "cookie", "id", "data", # ì¼ë°˜ì ì¸ íŒŒë¼ë¯¸í„° ì´ë¦„
                              "www"] or \
               len(word_lower) > 50 or \
               re.fullmatch(r'^[!@#$%^&*()_+=\[\]{}|;:\'",.<>?`~]$', word_lower) or \
               (word_lower.isdigit() and len(word_lower) < 4 and not re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', word_lower)) or \
               (word_lower in TRUSTED_DOMAINS_FOR_EXPLANATION and current_predicted_label == 'malicious' and weight > 0) : # ì‹ ë¢° ë„ë©”ì¸ì´ ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë˜ëŠ”ë° ê°•í•˜ê²Œ ê¸°ì—¬í•˜ëŠ” ê²½ìš° í•„í„°ë§ (ì˜¤ë¥˜ ë°©ì§€)
                return None, None
            
            display_text = ""
            meaning = ""
            
            # ì¸ìë¡œ ë°›ì€ URL íŒŒì‹± ì •ë³´ ì‚¬ìš©
            netloc_lower_local = netloc_lower_info
            tld_local = tld_info
            
            # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” TLD ëª©ë¡ (TRUSTED_DOMAINS_FOR_EXPLANATIONì—ì„œ ì¶”ì¶œ)
            trusted_tlds_from_config = [td.split('.')[-1] for td in TRUSTED_DOMAINS_FOR_EXPLANATION if '.' in td]
            
            
            
            if word_lower == 'noheader':
                display_text = f"**HTTP í—¤ë” ì •ë³´ ë¶€ì¬**"
                if current_predicted_label == 'malicious' and weight > 0: # ì•…ì„±ì¸ë° ê¸°ì—¬ë„ ì–‘ìˆ˜: í—¤ë” ë¶€ì¬ê°€ ì•…ì„± íŒë‹¨ì— ê¸ì •ì  ê¸°ì—¬
                    meaning = "URL ì ‘ì† ì‹œ **HTTP í—¤ë” ì •ë³´ê°€ ì „í˜€ ì—†ê±°ë‚˜ ë§¤ìš° ë¶ˆì™„ì „í•œ ê²½ìš°**, ì´ëŠ” ì„œë²„ê°€ ì •ë³´ë¥¼ ìˆ¨ê²¨ ë¶„ì„ì„ ì–´ë µê²Œ í•˜ê±°ë‚˜ ë¹„ì •ìƒì ì¸ ë™ì‘ì„ ì‹œë„í•  ìˆ˜ ìˆìŒì„ ë‚˜íƒ€ë‚´ **ì•…ì„±ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ì¤‘ìš”í•œ ê·¼ê±°**ê°€ ë©ë‹ˆë‹¤. ì •ìƒ ì›¹ì‚¬ì´íŠ¸ëŠ” ë‹¤ì–‘í•œ í—¤ë” ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
                elif current_predicted_label == 'benign' and weight < 0: # ì •ìƒì¸ë° ê¸°ì—¬ë„ ìŒìˆ˜: í—¤ë” ë¶€ì¬ê°€ ì •ìƒ íŒë‹¨ì— ë¶€ì •ì  ì˜í–¥
                    meaning = "ì´ URLì€ ì •ìƒìœ¼ë¡œ íŒë‹¨ë˜ì—ˆì§€ë§Œ, **HTTP í—¤ë” ì •ë³´ê°€ ì—†ê±°ë‚˜ ë§¤ìš° ë¶ˆì™„ì „í•˜ì—¬** ì •ìƒ íŒë‹¨ì— ë‹¤ì†Œ ë¶€ì •ì ì¸ ì˜í–¥ì„ ì£¼ì—ˆìŠµë‹ˆë‹¤. ì •ìƒì ì¸ ì›¹ì‚¬ì´íŠ¸ëŠ” ë³´í†µ ë‹¤ì–‘í•œ í—¤ë” ì •ë³´ë¥¼ ì£¼ê³ ë°›ìŠµë‹ˆë‹¤."
                else: # ê·¸ ì™¸ì˜ ê²½ìš° (ì˜ˆ: ì•…ì„±ì¸ë° ìŒìˆ˜ ê¸°ì—¬, ì •ìƒì¸ë° ì–‘ìˆ˜ ê¸°ì—¬ ë“±)
                    meaning = "ì´ URLì— ëŒ€í•œ HTTP í—¤ë” ì •ë³´ê°€ ì—†ë‹¤ëŠ” ì‚¬ì‹¤ì´ ëª¨ë¸ì˜ íŒë‹¨ì— ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤. í—¤ë” ì •ë³´ ë¶€ì¬ëŠ” ë•Œë•Œë¡œ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í™œë™ê³¼ ê´€ë ¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                return display_text, meaning
        
            # --- ê¸ì •ì  ê¸°ì—¬ ìš”ì¸ (ì •ìƒ URL íŒë‹¨ì— ì¤‘ìš”) ---
            if current_predicted_label == 'benign' and weight > 0:
                # HTTPS í”„ë¡œí† ì½œ
                if word_lower == 'https':
                    display_text = f"ì•ˆì „í•œ **HTTPS ì—°ê²°**"
                    meaning = "HTTPSëŠ” ì›¹ì‚¬ì´íŠ¸ì™€ ì‚¬ìš©ì ê°„ì˜ í†µì‹ ì„ ì•”í˜¸í™”í•˜ì—¬ ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ë³´í˜¸í•˜ëŠ” í”„ë¡œí† ì½œì…ë‹ˆë‹¤. ì´ URLì´ HTTPSë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì ì€ **ë³´ì•ˆì„±ì„ ë†’ì´ëŠ” ê¸ì •ì ì¸ ì‹ í˜¸**ë¡œ ì‘ìš©í–ˆìŠµë‹ˆë‹¤."
                # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ ë˜ëŠ” TLD
                elif any(td_part in word_lower for td_part in TRUSTED_DOMAINS_FOR_EXPLANATION) or \
                     any(td_part in netloc_lower_local for td_part in TRUSTED_DOMAINS_FOR_EXPLANATION) or \
                     (tld_local and (tld_local in trusted_tlds_from_config)):
                    display_text = f"'{word}'ì™€ ê°™ì€ **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ ë˜ëŠ” TLD**"
                    meaning = "ì´ URLì´ **ë„ë¦¬ ì•Œë ¤ì§„ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ ë˜ëŠ” TLD**ë¥¼ í¬í•¨í•˜ê³  ìˆì–´ ì •ìƒìœ¼ë¡œ íŒë‹¨ë  ê°€ëŠ¥ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤."
                
                elif abs(weight) > 0.05 and len(word_lower) > 1 and not word_lower.isdigit() and not re.fullmatch(r'.*\d{5,}.*', word_lower): # ë„ˆë¬´ ê¸´ ìˆ«ìì—´ ì œì™¸
                    display_text = f"URL ë‚´ '{word}' íŒ¨í„´"
                    meaning = f"URL ê²½ë¡œ ë˜ëŠ” ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì— í¬í•¨ëœ '{word}'ì™€ ê°™ì€ **ì¼ë°˜ì ì´ê³  ì˜ˆìƒ ê°€ëŠ¥í•œ íŒ¨í„´**ì´ URLì˜ ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ê¸ì •ì ì¸ ì‹ í˜¸ë¡œ ì‘ìš©í–ˆìŠµë‹ˆë‹¤."
            
            # --- ë¶€ì •ì  ê¸°ì—¬ ìš”ì¸ (ì•…ì„± URL íŒë‹¨ì— ì¤‘ìš”) ---
            elif current_predicted_label == 'malicious' and weight > 0:
                # HTTP í”„ë¡œí† ì½œ (HTTPSê°€ ì•„ë‹Œ ê²½ìš°)
                if word_lower == 'http': 
                    display_text = f"ë³´ì•ˆì— ì·¨ì•½í•œ **HTTP í”„ë¡œí† ì½œ**"
                    meaning = "HTTPëŠ” ì•”í˜¸í™”ë˜ì§€ ì•Šì€ í†µì‹  í”„ë¡œí† ì½œë¡œ ë°ì´í„°ê°€ ë…¸ì¶œë  ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. ì´ URLì´ HTTPë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì ì€ **ë³´ì•ˆ ì·¨ì•½ì„±ì„ ì‹œì‚¬í•˜ë©° ì•…ì„±ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ì£¼ëœ ê·¼ê±°** ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤."
                # IP ì£¼ì†Œ (ë„ë©”ì¸ ëŒ€ì‹  IP ì§ì ‘ ì‚¬ìš©)
                elif re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', word_lower):
                    display_text = f"URL ë‚´ **IP ì£¼ì†Œ ì§ì ‘ ì‚¬ìš©**"
                    meaning = "ì¼ë°˜ì ì¸ ì›¹ì‚¬ì´íŠ¸ëŠ” ë„ë©”ì¸ ì´ë¦„ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì´ URLì€ **IP ì£¼ì†Œë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì ‘ì†ì„ ìœ ë„**í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì¶”ì  íšŒí”¼ë‚˜ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ëª©ì ì„ ìˆ¨ê¸°ë ¤ëŠ” ì•…ì„± URLì˜ í”í•œ íŠ¹ì§•ì…ë‹ˆë‹¤."
                # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìµœìƒìœ„ ë„ë©”ì¸ (TLD)
                elif tld_local and (tld_local in ['vip', 'xin', 'top', 'xyz', 'online', 'loan', 'click', 'site', 'bid', 'asia', 'it'] or \
                                   (2 <= len(tld_local) <= 5 and not tld_local.isalnum() and not tld_local.isdigit() and tld_local not in trusted_tlds_from_config)): 
                    display_text = f"'.{tld_local}'ì™€ ê°™ì€ **ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìµœìƒìœ„ ë„ë©”ì¸**"
                    meaning = f"URLì˜ '.{tld_local}'ì™€ ê°™ì€ ìµœìƒìœ„ ë„ë©”ì¸(TLD)ì€ **ìŠ¤íŒ¸, í”¼ì‹±, ì•…ì„±ì½”ë“œ ìœ í¬ ë“±ì— ìì£¼ ì‚¬ìš©**ë˜ëŠ” ê²½í–¥ì´ ìˆì–´, ì´ URLì„ ì•…ì„±ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” **ê°•ë ¥í•œ ê·¼ê±°**ê°€ ë©ë‹ˆë‹¤."
                # ë¹„ì •ìƒì ì´ê±°ë‚˜ ë‚œë…í™”ëœ ì„œë¸Œë„ë©”ì¸/ê²½ë¡œ íŒ¨í„´ (ì˜ˆ: "8noX2wTHr", "com-xaaawn")
                elif (re.search(r'-\w{5,}|[a-f0-9]{8,}', word_lower) and len(word_lower) > 10) or \
                     (len(word_lower) > 15 and (word_lower.isdigit() or re.fullmatch(r'^[a-zA-Z0-9]+$', word_lower))) or \
                     re.search(r'\d{3,}\.\d{3,}\.\d{3,}', word_lower) or \
                     ('//' in word_lower and word_lower.count('/') > 3) or \
                     (re.search(r'([a-zA-Z0-9]{2,}\.){2,}[a-zA-Z]{2,}', word_lower) and not any(td in word_lower for td in TRUSTED_DOMAINS_FOR_EXPLANATION)): 
                    display_text = f"URL ë‚´ '{word}'ì™€ ê°™ì€ **ë¹„ì •ìƒì ì´ê±°ë‚˜ ë‚œë…í™”ëœ ë¬¸ìì—´/ì„œë¸Œë„ë©”ì¸**"
                    meaning = "ì´ URLì€ ì£¼ì†Œì— **ì˜ë¯¸ ì—†ëŠ” ê¸´ ë¬¸ìì—´, ë¬´ì‘ìœ„ ìˆ«ì/ë¬¸ì ì¡°í•©, ë¹„ì •ìƒì ì¸ ì„œë¸Œë„ë©”ì¸ êµ¬ì¡°** ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì˜ë„ë¥¼ ìˆ¨ê¸°ë ¤ í•˜ê³  ìˆìœ¼ë©°, ì´ëŠ” ì•…ì„± URLì˜ **ì „í˜•ì ì¸ íŠ¹ì§•**ì…ë‹ˆë‹¤."
                # ì¤‘ìš”í•œ HTTP í—¤ë” ì´ë¦„ ë° ê°’ (ì•…ì„±ìœ¼ë¡œ ê¸°ì—¬) - í•„ìš”ì‹œ ì¶”ê°€
                elif any(header_name.lower() in word_lower for header_name in IMPORTANT_HEADERS if word_lower != 'noheader'):
                    display_text = f"'{word}'ì™€ ê°™ì€ **ë¹„ì •ìƒì ì¸ HTTP í—¤ë” íŒ¨í„´**"
                    meaning = f"HTTP í—¤ë” '{word}'ì˜ ì¡´ì¬ ë˜ëŠ” ê°’ì´ ì¼ë°˜ì ì´ì§€ ì•Šê±°ë‚˜, **ì•…ì„± ì„œë²„ì—ì„œ í”íˆ ë°œê²¬ë˜ëŠ” íŒ¨í„´**ê³¼ ì¼ì¹˜í•˜ì—¬ URLì˜ ì•…ì„±ë„ë¥¼ ë†’ì´ëŠ” ìš”ì¸ìœ¼ë¡œ ì‘ìš©í–ˆìŠµë‹ˆë‹¤."
                # í”¼ì‹± ê´€ë ¨ í‚¤ì›Œë“œ (login, admin, bank, secure ë“±)
                elif word_lower in ['login', 'admin', 'bank', 'secure', 'update', 'verify', 'account'] and abs(weight) > 0.05:
                    display_text = f"URL ë‚´ '{word}'ì™€ ê°™ì€ **í”¼ì‹± ì˜ì‹¬ í‚¤ì›Œë“œ**"
                    meaning = f"ì´ URLì€ '{word}'ì™€ ê°™ì´ **ê¸ˆìœµ ê¸°ê´€, ë¡œê·¸ì¸ í˜ì´ì§€ ë“±ì„ ìœ„ì¥í•˜ë ¤ëŠ” í”¼ì‹± ê³µê²©ì— ìì£¼ ì‚¬ìš©ë˜ëŠ” í‚¤ì›Œë“œ**ë¥¼ í¬í•¨í•˜ê³  ìˆì–´ ì•…ì„±ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ì¤‘ìš”í•œ ê·¼ê±°ê°€ ë©ë‹ˆë‹¤."
                # ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•œ íŒŒì¼ í™•ì¥ì
                elif any(word_lower.endswith(ext) for ext in ['.exe', '.zip', '.rar', '.doc', '.docx', '.xls', '.xlsx', '.pdf', '.js']) and abs(weight) > 0.05:
                    display_text = f"URL ë‚´ '{word}'ì™€ ê°™ì€ **ì•…ì„± íŒŒì¼ í™•ì¥ì**"
                    meaning = f"ì´ URLì€ ì‹¤í–‰ íŒŒì¼ì´ë‚˜ ì••ì¶• íŒŒì¼, ë¬¸ì„œ íŒŒì¼ ë“± **ì•…ì„±ì½”ë“œ ìœ í¬ì— í”íˆ ì‚¬ìš©ë˜ëŠ” íŒŒì¼ í™•ì¥ì**ë¥¼ í¬í•¨í•˜ê³  ìˆì–´ ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë˜ëŠ” ìš”ì¸ì…ë‹ˆë‹¤."
                # ë‹¨ì¶• URL (short.ly/malware ì™€ ê°™ì´ ë‹¨ì¶• ë„ë©”ì¸ ìì²´)
                elif word_lower in ['short.ly', 'bit.ly', 'tinyurl.com', 'goo.gl', 'buff.ly', 'ow.ly', 't.co'] and abs(weight) > 0.05:
                    display_text = f"'{word}'ì™€ ê°™ì€ **ë‹¨ì¶• URL ë„ë©”ì¸**"
                    meaning = "ë‹¨ì¶• URLì€ ì‹¤ì œ ëª©ì ì§€ ì£¼ì†Œë¥¼ ìˆ¨ê²¨ ì•…ì„± ë§í¬ë¥¼ ìœ í¬í•˜ëŠ” ë° ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ URLì´ ë‹¨ì¶• URL ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í–ˆë‹¤ëŠ” ì ì´ ì•…ì„± íŒë‹¨ì— ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤."
                # ê¸°íƒ€ ì•…ì„±ìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ì¼ë°˜ íŒ¨í„´ (ê¸°ì—¬ë„ê°€ ë†’ê³  ìœ„ í•­ëª©ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
                elif abs(weight) > 0.05 and len(word_lower) > 1: # ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œì™¸
                    display_text = f"URL ë‚´ '{word}'ì™€ ê°™ì€ **ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´**"
                    meaning = "ì´ URL ë‚´ì— ëª¨ë¸ì´ ì•…ì„± URLì—ì„œ ìì£¼ ë°œê²¬í•œ ê²ƒìœ¼ë¡œ í•™ìŠµëœ **íŠ¹ì • ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¬¸ìì—´ íŒ¨í„´**ì´ í¬í•¨ë˜ì–´ ìˆì–´ ì•…ì„± íŒë‹¨ì— ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤."

            # --- ì •ìƒìœ¼ë¡œ íŒë‹¨ë˜ëŠ”ë° ë°©í•´í•˜ëŠ” ìš”ì¸ (ìŒìˆ˜ ê¸°ì—¬ë„) ---
            elif current_predicted_label == 'benign' and weight < 0:
                # ë³´ì•ˆì— ì·¨ì•½í•œ HTTP í”„ë¡œí† ì½œ (ì •ìƒìœ¼ë¡œ íŒë‹¨ë˜ëŠ”ë° ìŒìˆ˜ ê¸°ì—¬)
                if word_lower == 'http':
                    display_text = f"ë³´ì•ˆì— ì·¨ì•½í•œ **HTTP í”„ë¡œí† ì½œ**"
                    meaning = "ì´ URLì€ ì •ìƒìœ¼ë¡œ íŒë‹¨ë˜ì—ˆì§€ë§Œ, HTTPë¥¼ ì‚¬ìš©í•˜ì—¬ í†µì‹ ì´ ì•”í˜¸í™”ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì€ **ë³´ì•ˆ ì·¨ì•½ ìš”ì†Œë¡œ ì¸ì‹ë˜ì–´ ì •ìƒ íŒë‹¨ì„ ë‹¤ì†Œ ë°©í•´**í–ˆìŠµë‹ˆë‹¤. ìµœì‹  ì›¹ì‚¬ì´íŠ¸ëŠ” ëŒ€ë¶€ë¶„ HTTPSë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
                
                # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ì´ ì¡´ì¬í•˜ì§€ë§Œ ì •ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ ê²½ìš° (ì˜¤íƒ ê°€ëŠ¥ì„±)
                elif abs(weight) > 0.05 and len(word_lower) >= 5 and (word_lower.isdigit() or re.fullmatch(r'^[a-zA-Z0-9]+$', word_lower) or re.fullmatch(r'.*\d{5,}.*', word_lower)):
                    display_text = f"URL ë‚´ '{word}'ì™€ ê°™ì€ **ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´**"
                    meaning = "ì´ URLì€ ì •ìƒìœ¼ë¡œ íŒë‹¨ë˜ì—ˆì§€ë§Œ, URL ë‚´ì— **ì•…ì„± URLì—ì„œ í”íˆ ë°œê²¬ë˜ëŠ” ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¬¸ìì—´ íŒ¨í„´**ì´ í¬í•¨ë˜ì–´ ìˆì–´ ì •ìƒ íŒë‹¨ì„ ë‹¤ì†Œ ë°©í•´í•˜ëŠ” ìš”ì†Œë¡œ ì‘ìš©í–ˆìŠµë‹ˆë‹¤."
                # ì¿ í‚¤/ì„¸ì…˜ ì´ë¦„ì´ ìŒìˆ˜ ê¸°ì—¬ë¥¼ í•˜ëŠ” ê²½ìš°
                elif word_lower in ["aec", "nid", "sid", "phpsessid", "jsessionid", "ga"] and abs(weight) > 0.01:
                    display_text = f"'{word}'ì™€ ê°™ì€ **ì¼ë°˜ì ì¸ ì¿ í‚¤/ì„¸ì…˜ íŒ¨í„´**"
                    meaning = "ì´ URLì€ ì •ìƒìœ¼ë¡œ íŒë‹¨ë˜ì—ˆì§€ë§Œ, íŠ¹ì • ì¿ í‚¤/ì„¸ì…˜ ì´ë¦„ì´ **ì•…ì„± URLì—ì„œë„ ë°œê²¬ë˜ëŠ” ê²½ìš°ê°€ ìˆì–´** ëª¨ë¸ì˜ ì •ìƒ íŒë‹¨ì— ì•„ì£¼ ë¯¸ë¯¸í•˜ê²Œ ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì³¤ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

            # --- ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë˜ëŠ”ë° ì™„í™”í•˜ëŠ” ìš”ì¸ (ìŒìˆ˜ ê¸°ì—¬ë„) ---
            elif current_predicted_label == 'malicious' and weight < 0:
                # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ (ì•…ì„±ì¸ë° ìŒìˆ˜ ê¸°ì—¬)
                if any(td_part in word_lower for td_part in TRUSTED_DOMAINS_FOR_EXPLANATION) or \
                   any(td_part in netloc_lower_local for td_part in TRUSTED_DOMAINS_FOR_EXPLANATION): 
                    display_text = f"'{word}'ì™€ ê°™ì€ **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸**"
                    meaning = "ì´ URLì€ ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë˜ì—ˆì§€ë§Œ, **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸**ì´ í¬í•¨ë˜ì–´ ìˆì–´ ì•…ì„± íŒë‹¨ì„ ë‹¤ì†Œ ì™„í™”í•˜ëŠ” ìš”ì¸ìœ¼ë¡œ ì‘ìš©í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì•…ì„± URLì´ ì •ìƒ ì‚¬ì´íŠ¸ë¥¼ ëª¨ë°©í•˜ê±°ë‚˜ ë¦¬ë‹¤ì´ë ‰ì…˜ì„ í™œìš©í•˜ëŠ” ê²½ìš°ì— ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                # HTTPS í”„ë¡œí† ì½œ (ì•…ì„±ì¸ë° ìŒìˆ˜ ê¸°ì—¬)
                elif word_lower == 'https':
                    display_text = f"ì•ˆì „í•œ **HTTPS ì—°ê²°**"
                    meaning = "ì´ URLì€ ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë˜ì—ˆì§€ë§Œ, HTTPSë¥¼ ì‚¬ìš©í•˜ì—¬ í†µì‹ ì„ ì•”í˜¸í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì•…ì„± URLì´ **í•©ë²•ì ì¸ ê²ƒì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ë ¤ëŠ” ì‹œë„**ì¼ ìˆ˜ ìˆìœ¼ë©°, ì•…ì„± íŒë‹¨ì„ ë‹¤ì†Œ ì™„í™”í•˜ëŠ” ìš”ì¸ìœ¼ë¡œ ì‘ìš©í–ˆìŠµë‹ˆë‹¤."
                # WWW ì ‘ë‘ì‚¬ (ì•…ì„±ì¸ë° ìŒìˆ˜ ê¸°ì—¬, ê¸°ì—¬ë„ ë†’ì„ ë•Œë§Œ)
                elif word_lower == 'www' and abs(weight) > 0.05:
                    display_text = f"'WWW' ì ‘ë‘ì‚¬"
                    meaning = "ì´ URLì€ ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë˜ì—ˆì§€ë§Œ, 'WWW' ì ‘ë‘ì‚¬ê°€ í¬í•¨ë˜ì–´ ìˆì–´ ì•…ì„± íŒë‹¨ì„ ë‹¤ì†Œ ì™„í™”í•˜ëŠ” ìš”ì¸ìœ¼ë¡œ ì‘ìš©í–ˆìŠµë‹ˆë‹¤."
                # ê¸°íƒ€ ì •ìƒìœ¼ë¡œ íŒë‹¨ë˜ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆëŠ” ì¼ë°˜ íŒ¨í„´ (ì•…ì„±ì¸ë° ìŒìˆ˜ ê¸°ì—¬)
                elif abs(weight) > 0.05 and len(word_lower) > 1 and not word_lower.isdigit() and not re.fullmatch(r'.*\d{5,}.*', word_lower):
                    display_text = f"URL ë‚´ '{word}'ì™€ ê°™ì€ **ì •ìƒì ì¸ íŒ¨í„´**"
                    meaning = "ì´ URLì€ ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë˜ì—ˆì§€ë§Œ, URL ë‚´ì— **ì¼ë°˜ì ìœ¼ë¡œ ì •ìƒ URLì—ì„œ ë°œê²¬ë˜ëŠ” íŒ¨í„´**ì´ í¬í•¨ë˜ì–´ ìˆì–´ ì•…ì„± íŒë‹¨ì„ ë‹¤ì†Œ ì™„í™”í•˜ëŠ” ìš”ì†Œë¡œ ì‘ìš©í–ˆìŠµë‹ˆë‹¤."

            return display_text, meaning

        # --- ìƒì„¸ ì„¤ëª… ì¶œë ¥ í¬ë§¤íŒ… ---
        detailed_explanation_output = []
        detailed_explanation_output.append("\n--- ìƒì„¸ ë¶„ì„ (URL íŠ¹ì§•ë³„ ê¸°ì—¬ë„) ---")
        detailed_explanation_output.append("ğŸ’¡ ëª¨ë¸ì´ URLì„ ë¶„ì„í•˜ë©° ì¤‘ìš”í•˜ê²Œ íŒë‹¨í•œ ì£¼ìš” íŠ¹ì§•ë“¤ê³¼ ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.")
        detailed_explanation_output.append("   'ê¸°ì—¬ë„'ëŠ” ê° íŠ¹ì§•ì´ ìµœì¢… íŒë‹¨ì— ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ê¸°ì—¬í–ˆëŠ”ì§€ë¥¼ ìˆ«ìë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. (ê°’ì´ í´ìˆ˜ë¡ ê¸°ì—¬ë„ ë†’ìŒ, ìŒìˆ˜ì¼ìˆ˜ë¡ ì˜ˆì¸¡ì— ë°˜ëŒ€ë˜ëŠ” ì˜í–¥)\n")
        
        has_understandable_explanation = False
        significant_features_for_summary = [] # ìš”ì•½ì— ì‚¬ìš©ë  ì£¼ìš” íŠ¹ì§• ëª©ë¡

        # ì •ë ¬: ì ˆëŒ€ ê¸°ì—¬ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ
        sorted_explanation = sorted(explanation_list, key=lambda x: abs(x[1]), reverse=True)

        for word, weight in sorted_explanation:
            word_lower = word.lower() # ì—¬ê¸°ì„œ word_lowerë¥¼ ì •ì˜í•˜ê³  ì‚¬ìš©í•©ë‹ˆë‹¤.

            # LIMEì´ ë½‘ì•„ë‚¸ í† í°ì´ ì›ë˜ URLì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ë˜ëŠ” í—¤ë”ì—ì„œ ì˜¨ 'NOHEADER'ì¸ì§€ í™•ì¸
            is_from_url = word_lower in url.lower() or \
                          (parsed_url.netloc and word_lower in parsed_url.netloc.lower()) or \
                          (parsed_url.path and word_lower in parsed_url.path.lower()) or \
                          (parsed_url.query and word_lower in parsed_url.query.lower())
            
            is_noheader_word = (word_lower == 'noheader') # 'noheader'ì¸ ê²½ìš° ë³„ë„ í”Œë˜ê·¸

            # get_understandable_explanation_text_and_meaning í•¨ìˆ˜ í˜¸ì¶œ ì‹œ íŒŒì‹±ëœ ì •ë³´ë¥¼ ì¸ìë¡œ ì „ë‹¬
            desc_text, meaning = get_understandable_explanation_text_and_meaning(word, weight, predicted_label, url, parsed_url, netloc_lower, tld)
            
            # ìœ ì˜ë¯¸í•œ ê¸°ì—¬ë„ (ì ˆëŒ€ê°’ 0.01 ì´ìƒ)ì™€ ì„¤ëª…ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ í¬í•¨
            if desc_text and abs(weight) >= 0.01: # ë‚®ì€ ê¸°ì—¬ë„ í•„í„°ë§ ìœ ì§€
                # 'NOHEADER'ëŠ” URLì—ì„œ ì˜¨ ê²ƒì€ ì•„ë‹ˆë¯€ë¡œ is_from_urlì„ ë¬´ì‹œí•˜ê³  í•­ìƒ í¬í•¨
                if is_noheader_word:
                    detailed_explanation_output.append(f"  - **íŠ¹ì§•**: {desc_text} (ê¸°ì—¬ë„: {weight:.4f})")
                    if meaning:
                        detailed_explanation_output.append(f"    **ì„¤ëª…**: {meaning}\n")
                    has_understandable_explanation = True
                    # ìš”ì•½ì— ì‚¬ìš©ë  íŠ¹ì§• ì¶”ê°€ (ìµœëŒ€ 3ê°œ)
                    if len(significant_features_for_summary) < 3:
                        significant_features_for_summary.append("í—¤ë” ë¶€ì¬")
                    continue
                
                # URLì—ì„œ ì˜¨ íŠ¹ì§•ë§Œ ìœ ì˜ë¯¸í•˜ê²Œ ì„¤ëª… (í—¤ë” í‚¤/ê°’ ì œì™¸)
                # IMPORTANT_HEADERSëŠ” í—¤ë” ì •ë³´ì—ì„œ ì˜¤ëŠ” ê²ƒì´ë¯€ë¡œ is_from_url ì¡°ê±´ì—ì„œ ì œì™¸
                elif (is_from_url or any(h.lower() in word_lower for h in IMPORTANT_HEADERS)) and \
                     not any(h.lower() == word_lower for h in IMPORTANT_HEADERS) and \
                     not (word_lower in TRUSTED_DOMAINS_FOR_EXPLANATION and predicted_label == 'malicious' and weight > 0) : # ì‹ ë¢° ë„ë©”ì¸ì´ ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë˜ëŠ”ë° ê°•í•˜ê²Œ ê¸°ì—¬í•˜ëŠ” ê²½ìš° ì œì™¸
                    
                    detailed_explanation_output.append(f"  - **íŠ¹ì§•**: {desc_text} (ê¸°ì—¬ë„: {weight:.4f})")
                    if meaning:
                        detailed_explanation_output.append(f"    **ì„¤ëª…**: {meaning}\n")
                    has_understandable_explanation = True

                    # ìš”ì•½ì— ì‚¬ìš©ë  íŠ¹ì§• ì¶”ê°€ (ìµœëŒ€ 3ê°œ)
                    if len(significant_features_for_summary) < 3:
                        if "HTTPS ì—°ê²°" in desc_text:
                            significant_features_for_summary.append("https")
                        elif "HTTP í”„ë¡œí† ì½œ" in desc_text:
                            significant_features_for_summary.append("http")
                        elif "ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸" in desc_text:
                            # 'google.com' ë˜ëŠ” 'google'ë§Œ ë‚¨ê¸°ë„ë¡
                            if '.' in word: # wordëŠ” ì›ë³¸ ë‹¨ì–´ (ì†Œë¬¸ìX)
                                try: # URL íŒŒì‹± ì‹œë„ (wordê°€ ìœ íš¨í•œ ë„ë©”ì¸ í˜•íƒœì¼ ê²½ìš°)
                                    parsed_sum_domain = urlparse("https://" + word).netloc
                                    significant_features_for_summary.append(parsed_sum_domain)
                                except ValueError: # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ word ì‚¬ìš©
                                    significant_features_for_summary.append(word)
                            else:
                                significant_features_for_summary.append(word) # wordëŠ” ì›ë³¸ ë‹¨ì–´ (ì†Œë¬¸ì X)
                        elif "'WWW' ì ‘ë‘ì‚¬" in desc_text:
                            significant_features_for_summary.append("www")
                        elif "IP ì£¼ì†Œ ì§ì ‘ ì‚¬ìš©" in desc_text:
                            significant_features_for_summary.append("IP ì£¼ì†Œ ì‚¬ìš©")
                        elif "ë¹„ì •ìƒì ì´ê±°ë‚˜ ë‚œë…í™”ëœ ë¬¸ìì—´" in desc_text or "ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´" in desc_text or "ì˜ë¯¸ ì—†ëŠ” ê¸´ ë¬¸ìì—´" in desc_text:
                            if len(word) > 15:
                                significant_features_for_summary.append(word[:12] + "...") 
                            else:
                                significant_features_for_summary.append(word)
                        elif "ì¿ í‚¤/ì„¸ì…˜ íŒ¨í„´" in desc_text: # ì´ í•­ëª©ì€ ì„¤ëª…ì„ ìœ„í•´ ìœ ì§€í•˜ë˜ ìš”ì•½ì—ëŠ” í¬í•¨í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ê³ ë ¤
                            # significant_features_for_summary.append(f"{word} (ì¿ í‚¤ íŒ¨í„´)")
                            pass
                        elif "ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìµœìƒìœ„ ë„ë©”ì¸" in desc_text or "íŠ¹ì´í•œ ìµœìƒìœ„ ë„ë©”ì¸" in desc_text:
                            tld_extracted_from_desc = re.search(r"'\.([^']+)'", desc_text)
                            if tld_extracted_from_desc:
                                significant_features_for_summary.append(f".{tld_extracted_from_desc.group(1)}")
                            else:
                                significant_features_for_summary.append(word)
                        elif "í”¼ì‹± ì˜ì‹¬ í‚¤ì›Œë“œ" in desc_text:
                            significant_features_for_summary.append(word)
                        elif "ì•…ì„± íŒŒì¼ í™•ì¥ì" in desc_text:
                            significant_features_for_summary.append(word)
                        elif "ë‹¨ì¶• URL ë„ë©”ì¸" in desc_text:
                            significant_features_for_summary.append(word)
                        else: # ìœ„ ë¶„ë¥˜ì— ì†í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ì ì¸ ìœ ì˜ë¯¸í•œ ë‹¨ì–´
                            significant_features_for_summary.append(word)
                
        if not has_understandable_explanation:
            detailed_explanation_output.append("   ëª¨ë¸ì´ íŒë‹¨ì— ì‚¬ìš©í•œ ì£¼ìš” íŠ¹ì§•ì´ ëª…í™•í•˜ê²Œ í•„í„°ë§ë˜ì§€ ì•Šê±°ë‚˜, ê¸°ì—¬ë„ê°€ ë‚®ì€ íŠ¹ì§•ë“¤ì´ ëŒ€ë¶€ë¶„ì…ë‹ˆë‹¤.")
            detailed_explanation_output.append("   ì´ëŠ” URLì˜ íŠ¹ì§•ì´ ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ì™€ ìœ ì‚¬í•˜ì§€ ì•Šê±°ë‚˜, ë¯¸ë¬˜í•œ íŒ¨í„´ë“¤ë¡œ ì´ë£¨ì–´ì§„ ê²½ìš° ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")

        detailed_explanation_output.append("----------------------------")
        detailed_explanation_output.append("ğŸ’¡ ì´ ë¶„ì„ì€ ëª¨ë¸ì´ í•™ìŠµí•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ë©°, ëª¨ë“  URLì— ëŒ€í•œ ì ˆëŒ€ì ì¸ íŒë‹¨ ê¸°ì¤€ì€ ì•„ë‹™ë‹ˆë‹¤. ")
        detailed_explanation_output.append("   ì˜ì‹¬ìŠ¤ëŸ¬ìš´ URLì€ ì§ì ‘ ì ‘ì†í•˜ê¸° ì „ ë°˜ë“œì‹œ ì£¼ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n")

        # --- ìš”ì•½ ì„¤ëª… ìƒì„± (reason_summary) ---
        reason_phrases = []
        if predicted_label == 'malicious':
            if significant_features_for_summary:
                malicious_patterns_display = []
                mitigating_patterns_display = []

                for feat in set(significant_features_for_summary): # ìš”ì•½ íŠ¹ì§• ì¤‘ë³µ ì œê±°
                    corresponding_weight = None
                    for lime_word, lime_weight in explanation_list:
                        # ìš”ì•½ìš© featê³¼ LIMEì˜ wordë¥¼ ë§¤ì¹­í•˜ëŠ” ë” ê²¬ê³ í•œ ë¡œì§
                        # word_lowerë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„êµ ì¼ê´€ì„± í™•ë³´
                        lime_word_lower = lime_word.lower()
                        if feat.lower() == lime_word_lower or \
                           (feat.endswith('...') and lime_word_lower.startswith(feat[:-3].lower())) or \
                           (feat.startswith('.') and f".{lime_word_lower}" == feat) or \
                           (feat == "í—¤ë” ë¶€ì¬" and lime_word_lower == 'noheader') or \
                           (feat == "IP ì£¼ì†Œ ì‚¬ìš©" and re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', lime_word_lower)) or \
                           (feat == "https" and lime_word_lower == 'https') or \
                           (feat == "http" and lime_word_lower == 'http') or \
                           (feat == "www" and lime_word_lower == 'www') :
                            corresponding_weight = lime_weight
                            break
                    
                    if corresponding_weight is not None:
                        if corresponding_weight > 0: # ì•…ì„±ìœ¼ë¡œ ê°•í•˜ê²Œ ê¸°ì—¬
                            malicious_patterns_display.append(feat)
                        else: # ì•…ì„±ì¸ë° ì™„í™” ê¸°ì—¬ (ìŒìˆ˜ ê¸°ì—¬ë„)
                            mitigating_patterns_display.append(feat)
                    # else: # LIME explanation_listì—ì„œ ê¸°ì—¬ë„ë¥¼ ì°¾ì§€ ëª»í•˜ë©´, ì¼ë‹¨ ì•…ì„± íŒ¨í„´ìœ¼ë¡œ ê°„ì£¼ (ë°©ì–´ì  ì½”ë“œ - ìœ„ ë§¤ì¹­ ë¡œì§ ê°œì„ ìœ¼ë¡œ ì´ elseëŠ” ê±°ì˜ ì‹¤í–‰ ì•ˆ ë¨)
                    #     malicious_patterns_display.append(feat) 

                if malicious_patterns_display:
                    reason_phrases.append(f"íŠ¹íˆ '{', '.join(sorted(list(set(malicious_patterns_display))))}' ë“±ê³¼ ê°™ì´ **ì •ìƒì ì´ì§€ ì•Šê±°ë‚˜ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´**ì´ ë°œê²¬ë˜ì–´ ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                if mitigating_patterns_display:
                    reason_phrases.append(f"ì´ URLì€ '{', '.join(sorted(list(set(mitigating_patterns_display))))}'ì™€ ê°™ì€ ì¼ë°˜ì ì¸ íŒ¨í„´ì„ í¬í•¨í•˜ê³  ìˆìœ¼ë‚˜, ì „ë°˜ì ìœ¼ë¡œ ì•…ì„± ìœ„í—˜ì´ ë†’ì€ ê²ƒìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                if not reason_phrases:
                    reason_phrases.append("URLì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ì™€ íŒ¨í„´ì´ ì•Œë ¤ì§„ ì•…ì„± URLê³¼ ìœ ì‚¬í•˜ì—¬ ì˜ì‹¬ë©ë‹ˆë‹¤.")

            else: # significant_features_for_summaryê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
                reason_phrases.append("URLì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ì™€ íŒ¨í„´ì´ ì•Œë ¤ì§„ ì•…ì„± URLê³¼ ìœ ì‚¬í•˜ì—¬ ì˜ì‹¬ë©ë‹ˆë‹¤.")
                
        else: # 'benign'
            if significant_features_for_summary:
                benign_patterns_display = []
                negative_patterns_display = []

                for feat in set(significant_features_for_summary): # ìš”ì•½ íŠ¹ì§• ì¤‘ë³µ ì œê±°
                    corresponding_weight = None
                    for lime_word, lime_weight in explanation_list:
                        # ìš”ì•½ìš© featê³¼ LIMEì˜ wordë¥¼ ë§¤ì¹­í•˜ëŠ” ë” ê²¬ê³ í•œ ë¡œì§
                        # word_lowerë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„êµ ì¼ê´€ì„± í™•ë³´
                        lime_word_lower = lime_word.lower()
                        if feat.lower() == lime_word_lower or \
                           (feat.endswith('...') and lime_word_lower.startswith(feat[:-3].lower())) or \
                           (feat.startswith('.') and f".{lime_word_lower}" == feat) or \
                           (feat == "í—¤ë” ë¶€ì¬" and lime_word_lower == 'noheader') or \
                           (feat == "IP ì£¼ì†Œ ì‚¬ìš©" and re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', lime_word_lower)) or \
                           (feat == "https" and lime_word_lower == 'https') or \
                           (feat == "http" and lime_word_lower == 'http') or \
                           (feat == "www" and lime_word_lower == 'www') :
                            corresponding_weight = lime_weight
                            break

                    if corresponding_weight is not None:
                        if corresponding_weight > 0: # ì •ìƒìœ¼ë¡œ ê°•í•˜ê²Œ ê¸°ì—¬
                            benign_patterns_display.append(feat)
                        else: # ì •ìƒì¸ë° ë°©í•´ ê¸°ì—¬ (ìŒìˆ˜ ê¸°ì—¬ë„)
                            negative_patterns_display.append(feat)
                    # else: # LIME explanation_listì—ì„œ ê¸°ì—¬ë„ë¥¼ ì°¾ì§€ ëª»í•˜ë©´, ì¼ë‹¨ ì •ìƒ íŒ¨í„´ìœ¼ë¡œ ê°„ì£¼
                    #     benign_patterns_display.append(feat) 
                
                # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
                benign_patterns_display = sorted(list(set(benign_patterns_display)))
                negative_patterns_display = sorted(list(set(negative_patterns_display)))


                if benign_patterns_display:
                    reason_phrases.append(f"ì´ URLì€ '{', '.join(benign_patterns_display)}' ë“±ê³¼ ê°™ì´ **ì¼ë°˜ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŒ¨í„´**ì„ ê°€ì§€ê³  ìˆì–´ ì•ˆì „í•©ë‹ˆë‹¤.")
                
                if negative_patterns_display:
                    reason_phrases.append(f"ì¼ë¶€ '{', '.join(negative_patterns_display)}'ì™€ ê°™ì€ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆìœ¼ë‚˜, ì „ë°˜ì ì¸ êµ¬ì¡°ê°€ ì•ˆì „í•œ ê²ƒìœ¼ë¡œ íŒë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                if not reason_phrases:
                    reason_phrases.append("URLì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ì™€ íŒ¨í„´ì´ ì•Œë ¤ì§„ ì •ìƒ URLê³¼ ìœ ì‚¬í•˜ì—¬ ì•ˆì „í•©ë‹ˆë‹¤.")
            else: # significant_features_for_summaryê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°
                reason_phrases.append("URLì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ì™€ íŒ¨í„´ì´ ì•Œë ¤ì§„ ì •ìƒ URLê³¼ ìœ ì‚¬í•˜ì—¬ ì•ˆì „í•©ë‹ˆë‹¤.")

        reason_summary = f"ì´ URLì€ **{predicted_label.upper()}**ë¡œ íŒë‹¨ë©ë‹ˆë‹¤. " + " ".join(reason_phrases)

    except Exception as e:
        print(f"LIME ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        reason_summary = f"ì´ URLì€ **{predicted_label.upper()}**ë¡œ íŒë‹¨ë©ë‹ˆë‹¤. íŒë‹¨ ê·¼ê±°ë¥¼ ì„¤ëª…í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        explanation_list = []
        detailed_explanation_output = ["LIME ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: " + str(e)]

    return {
        "reason_summary": reason_summary,
        "detailed_explanation_list": explanation_list, 
        "formatted_detailed_explanation": "\n".join(detailed_explanation_output)
    }

# --- 3. URL ë¶„ë¥˜ ë° ì„¤ëª…ì„ í†µí•©í•˜ëŠ” í•¨ìˆ˜ ---
def classify_url_and_explain(url: str, model, tokenizer) -> dict:
    # 1) URL ì˜ˆì¸¡ ìˆ˜í–‰
    pred_out = predict_url(url, model, tokenizer)

    # 2) LIME ì„¤ëª… ìƒì„±
    lime_out = explain_prediction_with_lime(
        url,
        pred_out["header_info"],
        model,
        tokenizer,
        pred_out["predicted_class_id"]
    )


    # 3) DB ì €ì¥ìš© í•„ë“œëª…ì— ë§ì¶°ì„œ dict ë°˜í™˜
    is_mal = 1 if pred_out["predicted_label"] == "malicious" else 0

    return {
        "url": url,
        "header_info": pred_out["header_info"],
        "is_malicious": is_mal,
        "confidence": pred_out["confidence"],    # float íƒ€ì…
        "true_label": None,                      
        "reason_summary": lime_out["reason_summary"],
        "detailed_explanation": lime_out["formatted_detailed_explanation"]
    }
