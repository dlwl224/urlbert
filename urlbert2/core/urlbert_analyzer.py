# /home/kong/urlbert/url_bert/urlbert2/core/url_analyzer.py

import os
import sys

# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ (core)
current_dir = os.path.dirname(os.path.abspath(__file__))
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ (urlbert2)
project_root = os.path.abspath(os.path.join(current_dir, '..'))

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²€ìƒ‰ ê²½ë¡œì— ì¶”ê°€
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import torch
import torch.nn.functional as F
import requests
import random
import numpy as np

from pytorch_pretrained_bert import BertTokenizer 
from lime.lime_text import LimeTextExplainer

from config import (
    PAD_SIZE, DEVICE, CLASS_LABELS, IMPORTANT_HEADERS,
    REQUEST_TIMEOUT_SECONDS, LIME_NUM_FEATURES, LIME_NUM_SAMPLES,
    TRUSTED_DOMAINS_FOR_EXPLANATION
)

# --- HTTP í—¤ë” ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜ ---
def get_header_info(url: str) -> str:
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/114 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 Version/14.0.3 Safari/605.1.15",
        "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 Version/14.0 Mobile/15E148 Safari/604.1"
    ]
    headers = {
        "User-Agent": random.choice(user_agents),
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": "https://www.google.com/"
    }

    try:
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT_SECONDS, allow_redirects=True)
        resp_headers = response.headers
        
        # IMPORTANT_HEADERSê°€ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜ë˜ì–´ ìˆë‹¤ë©´
        important = {
            k: resp_headers.get(k, "") for k in IMPORTANT_HEADERS
        } 
        header_str = ", ".join(f"{k}: {v}" for k, v in important.items() if v)
        return header_str if header_str else "NOHEADER"
    except requests.exceptions.RequestException:
        return "NOHEADER"
    except Exception:
        return "NOHEADER"

# --- ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜  ---
def preprocess_url_for_inference(url: str, header_info: str, tokenizer: BertTokenizer, pad_size: int = PAD_SIZE):
    text = f"{url} [SEP] {header_info}"
    tokenized_text = tokenizer.tokenize(text)
    
    tokens = ["[CLS]"] + tokenized_text + ["[SEP]"]
    ids = tokenizer.convert_tokens_to_ids(tokens)
    types = [0] * (len(ids)) 
    masks = [1] * len(ids)

    if len(ids) < pad_size:
        types = types + [1] * (pad_size - len(ids)) 
        masks = masks + [0] * (pad_size - len(ids))
        ids = ids + [0] * (pad_size - len(ids))
    else:
        types = types[:pad_size]
        masks = masks[:pad_size]
        ids = ids[:pad_size]

    assert len(ids) == len(masks) == len(types) == pad_size

    return (
        torch.tensor([ids], dtype=torch.long).to(DEVICE),
        torch.tensor([types], dtype=torch.long).to(DEVICE),
        torch.tensor([masks], dtype=torch.long).to(DEVICE)
    )

# --- LIME ì„¤ëª…ìë¥¼ ìœ„í•œ ì˜ˆì¸¡ í•¨ìˆ˜  ---
def lime_predictor_fn(texts, model, tokenizer): 
    probabilities = []
    for text_input in texts:
        parts = text_input.split(" [SEP] ", 1)
        url_part = parts[0]
        header_part = parts[1] if len(parts) > 1 else "NOHEADER"

        input_ids, input_types, input_masks = preprocess_url_for_inference(
            url_part, header_part, tokenizer, PAD_SIZE
        )

        with torch.no_grad():
            outputs = model([input_ids, input_types, input_masks])
            probs = F.softmax(outputs, dim=1).cpu().numpy()
            probabilities.append(probs[0])
    return np.array(probabilities)

# --- URL ë¶„ë¥˜ ë° ì„¤ëª… ìƒì„± í•¨ìˆ˜ ---
def classify_url_with_explanation(url: str, model, tokenizer) -> dict: 
    print(f"\nURL ë¶„ì„ ì‹œì‘: {url}")
    header_info = get_header_info(url)
    print(f"ì¶”ì¶œëœ í—¤ë” ì •ë³´: {header_info if header_info != 'NOHEADER' else 'ì—†ìŒ'}")

    input_ids, input_types, input_masks = preprocess_url_for_inference(
        url, header_info, tokenizer, PAD_SIZE
    )

    with torch.no_grad():
        outputs = model([input_ids, input_types, input_masks])
        probabilities = F.softmax(outputs, dim=1)
        predicted_class_id = torch.argmax(probabilities, dim=1).item()

    predicted_label = CLASS_LABELS[predicted_class_id]
    confidence = probabilities[0][predicted_class_id].item() * 100

    full_text_for_lime = f"{url} [SEP] {header_info}"

    try:
        explainer = LimeTextExplainer(class_names=list(CLASS_LABELS.values()))

        explanation = explainer.explain_instance(
            full_text_for_lime,
            classifier_fn=lambda texts: lime_predictor_fn(texts, model, tokenizer),
            labels=[0, 1], 
            num_features=LIME_NUM_FEATURES, 
            num_samples=LIME_NUM_SAMPLES 
        )

        explanation_list = explanation.as_list(label=predicted_class_id)
        
        # --- LIME ì„¤ëª… ìš”ì•½ ë¡œì§ ê°œì„  ---
        reason_phrases = []
        
        # ì´í•´í•˜ê¸° ì‰¬ìš´ í‚¤ì›Œë“œë¥¼ í•„í„°ë§í•˜ê³  ì„¤ëª… ë¬¸êµ¬ë¥¼ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
        # ë°˜í™˜ê°’ì€ (ì„¤ëª… ë¬¸êµ¬, í•´ë‹¹ íŠ¹ì§•ì˜ ì¼ë°˜ì ì¸ ì˜ë¯¸ ì„¤ëª…) íŠœí”Œ
        def get_understandable_explanation_text_and_meaning(word, weight, predicted_label_context_for_meaning):
            # 1ë‹¨ê³„ í•„í„°ë§: BERT íŠ¹ìˆ˜ í† í°, ë„ˆë¬´ ê¸´ ë¬¸ìì—´, ë¶ˆí•„ìš”í•œ ì¿ í‚¤/ì„¸ì…˜ ì´ë¦„
            # word.strip().lower()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–´ë–¤ í˜•íƒœì˜ SEPë„ í™•ì‹¤íˆ ê±¸ëŸ¬ëƒ„
            if word.strip().lower() in ["[sep]", "[cls]", "[pad]"] or \
               len(word) > 50 or \
               word.upper() in ["NID", "SID", "PHPSESSID", "AEC"]:
                return None, None

            # 2ë‹¨ê³„: ì˜í–¥ë„ì— ë”°ë¥¸ ì„¤ëª… ë¬¸êµ¬ì™€ ì˜ë¯¸ ìƒì„±
            display_text = ""
            meaning = ""
            
            # ì˜ˆì¸¡ëœ ë¼ë²¨ì— ëŒ€í•œ ê¸°ì—¬ ë°©í–¥
            # LIMEì˜ weightëŠ” í•´ë‹¹ í´ë˜ìŠ¤ë¡œì˜ ê¸°ì—¬ë„.
            # ì¦‰, 'benign'ì¼ ë•Œ ì–‘ìˆ˜ weightëŠ” benignìœ¼ë¡œ ê°€ëŠ” ë° ê¸ì •ì , ìŒìˆ˜ weightëŠ” benignìœ¼ë¡œ ê°€ëŠ” ë° ë¶€ì •ì (maliciousë¡œ ìœ ë„)
            # 'malicious'ì¼ ë•Œ ì–‘ìˆ˜ weightëŠ” maliciousë¡œ ê°€ëŠ” ë° ê¸ì •ì , ìŒìˆ˜ weightëŠ” maliciousë¡œ ê°€ëŠ” ë° ë¶€ì •ì (benignìœ¼ë¡œ ìœ ë„)
            
            # --- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸ ---
            if word.lower() in TRUSTED_DOMAINS_FOR_EXPLANATION:
                meaning = "ì›¹ì‚¬ì´íŠ¸ì˜ ì£¼ì†Œ(ë„ë©”ì¸)ëŠ” í•´ë‹¹ ì›¹ì‚¬ì´íŠ¸ì˜ ì‹ ë¢°ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤. ë„ë¦¬ ì•Œë ¤ì§€ê³  ì•ˆì „í•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ë„ë©”ì¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „í•œ URLë¡œ íŒë‹¨ë  ê°€ëŠ¥ì„±ì„ ë†’ì…ë‹ˆë‹¤."
                if predicted_label_context_for_meaning == "ì •ìƒìœ¼ë¡œ íŒë‹¨ë  ê°€ëŠ¥ì„±ì„" and weight > 0:
                    display_text = f"'{word}'ì™€ ê°™ì€ **ë„ë¦¬ ì•Œë ¤ì§€ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸**ì´ ì •ìƒìœ¼ë¡œ íŒë‹¨ë  ê°€ëŠ¥ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤."
                elif predicted_label_context_for_meaning == "ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë  ê°€ëŠ¥ì„±ì„" and weight < 0: # ì•…ì„±ì¸ë° ìŒìˆ˜ ì˜í–¥ -> ì•…ì„± íŒë‹¨ì„ ê°ì†Œ ì‹œí‚´ (ì¦‰, ì •ìƒìª½ìœ¼ë¡œ ë¯¸ëŠ” ìš”ì†Œ)
                    display_text = f"'{word}'ì™€ ê°™ì€ **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸**ì´ ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë  ê°€ëŠ¥ì„±ì„ ë‹¤ì†Œ ê°ì†Œì‹œì¼°ìŠµë‹ˆë‹¤. ì´ëŠ” ì•…ì„± URLì´ ì •ìƒ ì‚¬ì´íŠ¸ë¥¼ ëª¨ë°©í•˜ë ¤ í•  ë•Œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆëŠ” íŒ¨í„´ì…ë‹ˆë‹¤."
                else: # ê·¸ ì™¸ì˜ ê²½ìš° (ì˜ˆ: ì •ìƒì¸ë° ìŒìˆ˜ ì˜í–¥ - ì´ ê²½ìš°ëŠ” 'ì •ìƒìœ¼ë¡œ íŒë‹¨ë  ê°€ëŠ¥ì„±ì„ ê°ì†Œì‹œì¼°ë‹¤'ê°€ ë” ëª…í™•)
                    display_text = f"'{word}'ì™€ ê°™ì€ **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë„ë©”ì¸**ì´ {predicted_label_context_for_meaning} {('ì¦ê°€ì‹œí‚¤ëŠ”' if weight > 0 else 'ê°ì†Œì‹œí‚¤ëŠ”')} ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤."
            
            # --- HTTPS/HTTP ---
            elif word.lower() == 'https':
                meaning = "HTTPSëŠ” ì›¹ì‚¬ì´íŠ¸ì™€ ì‚¬ìš©ì ê°„ì˜ í†µì‹ ì„ ì•”í˜¸í™”í•˜ì—¬ ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ë³´í˜¸í•˜ëŠ” í”„ë¡œí† ì½œì…ë‹ˆë‹¤. HTTPS ì‚¬ìš©ì€ URLì˜ ë³´ì•ˆ ìˆ˜ì¤€ì„ ë†’ì´ëŠ” ê¸ì •ì ì¸ ì‹ í˜¸ì…ë‹ˆë‹¤."
                display_text = f"ì•ˆì „í•œ **HTTPS ì—°ê²°**ì´ {predicted_label_context_for_meaning} {('ì¦ê°€ì‹œí‚¤ëŠ”' if weight > 0 else 'ê°ì†Œì‹œí‚¤ëŠ”')} ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤."
            elif word.lower() == 'http':
                meaning = "HTTPëŠ” ì•”í˜¸í™”ë˜ì§€ ì•Šì€ í†µì‹  í”„ë¡œí† ì½œë¡œ, ë°ì´í„°ê°€ ë…¸ì¶œë  ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. ìµœì‹  ì›¹ì‚¬ì´íŠ¸ëŠ” ëŒ€ë¶€ë¶„ HTTPSë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, HTTPë§Œ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìš”ì†Œë¡œ ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                display_text = f"ë³´ì•ˆì— ì·¨ì•½í•œ 'HTTP' í”„ë¡œí† ì½œì´ {predicted_label_context_for_meaning} {('ì¦ê°€ì‹œí‚¤ëŠ”' if weight > 0 else 'ê°ì†Œì‹œí‚¤ëŠ”')} ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤."

            # --- WWW ---
            elif word.lower() == 'www':
                meaning = "ëŒ€ë¶€ë¶„ì˜ ì¼ë°˜ì ì¸ ì›¹ì‚¬ì´íŠ¸ëŠ” 'www' ì ‘ë‘ì‚¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” í‘œì¤€ì ì¸ ì›¹ ì£¼ì†Œ í˜•íƒœë¡œ, URLì˜ ì •ìƒì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì‹ í˜¸ë¡œ ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                display_text = f"'WWW' ì ‘ë‘ì‚¬ ì‚¬ìš©ì´ {predicted_label_context_for_meaning} {('ì¦ê°€ì‹œí‚¤ëŠ”' if weight > 0 else 'ê°ì†Œì‹œí‚¤ëŠ”')} ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤."
            
            # --- NOHEADER --- 
            elif word.lower() == 'noheader':
                meaning = "URL ì ‘ì† ì‹œ **HTTP í—¤ë” ì •ë³´ê°€ ì „í˜€ ì—†ê±°ë‚˜ ë¹„ì •ìƒì ì¸ ê²½ìš°**, ì´ëŠ” ì„œë²„ ì„¤ì •ì˜ ë¬¸ì œì´ê±°ë‚˜, ì •ë³´ë¥¼ ìˆ¨ê²¨ ë¶„ì„ì„ ì–´ë µê²Œ í•˜ë ¤ëŠ” ì•…ì˜ì ì¸ ì‹œë„ë¡œ í•´ì„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì •ìƒì ì¸ ì›¹ì‚¬ì´íŠ¸ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ì–‘í•œ í—¤ë” ì •ë³´ë¥¼ ì£¼ê³ ë°›ìŠµë‹ˆë‹¤."
                display_text = f"**HTTP í—¤ë” ì •ë³´ ë¶€ì¬**ê°€ {predicted_label_context_for_meaning} {('ì¦ê°€ì‹œí‚¤ëŠ”' if weight > 0 else 'ê°ì†Œì‹œí‚¤ëŠ”')} ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤."
            
            # --- ì¤‘ìš” HTTP í—¤ë” ì´ë¦„ --- 
            elif word in IMPORTANT_HEADERS: 
                meaning = f"'{word}' í—¤ë”ëŠ” ì›¹ ì„œë²„ì™€ í´ë¼ì´ì–¸íŠ¸ ê°„ì˜ í†µì‹  ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ì´ í—¤ë”ì˜ ë‚´ìš©ì´ë‚˜ ì¡´ì¬ ì—¬ë¶€ëŠ” ì›¹ì‚¬ì´íŠ¸ì˜ íŠ¹ì„±(ì˜ˆ: ì‚¬ìš©ëœ ì›¹ ì„œë²„ ì¢…ë¥˜, ì½˜í…ì¸  íƒ€ì…, ì¿ í‚¤ ì„¤ì • ë“±)ì„ íŒŒì•…í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤. ì•…ì„± ì‚¬ì´íŠ¸ì˜ ê²½ìš° ì •ìƒì ì¸ í—¤ë”ê°€ ì—†ê±°ë‚˜, í”¼ì‹±ì„ ìœ„í•´ íŠ¹ì • í—¤ë”ë¥¼ ì¡°ì‘í•˜ëŠ” ë“± ë¹„ì •ìƒì ì¸ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                display_text = f"'{word}' í—¤ë”ì˜ íŠ¹ì • ê°’ì´ {predicted_label_context_for_meaning} {('ì¦ê°€ì‹œí‚¤ëŠ”' if weight > 0 else 'ê°ì†Œì‹œí‚¤ëŠ”')} ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤."
            
            # --- ì¼ë°˜ì ì¸ URL êµ¬ì„± ìš”ì†Œ (ë„ë©”ì¸, ê²½ë¡œ ì¡°ê°, ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ë“±) ---
            # ìˆ«ìë¡œë§Œ ì´ë£¨ì–´ì§„ ë‹¨ì–´ëŠ” ë„ˆë¬´ í”í•˜ê³  ì˜ë¯¸ ì—†ìœ¼ë¯€ë¡œ ì œì™¸ 
            elif ('.' in word and len(word) > 2) or \
                 (len(word) <= 30 and all(c.isalnum() or c in ['-', '_', '%', '/', '.'] for c in word) and not word.isdigit()):
                meaning = "URLì˜ ê²½ë¡œë‚˜ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì— í¬í•¨ëœ ë¬¸ìì—´ íŒ¨í„´ì€ ì•…ì„± í–‰ìœ„(ì˜ˆ: í”¼ì‹±, ë©€ì›¨ì–´ ë°°í¬)ë¥¼ ìˆ¨ê¸°ê±°ë‚˜ ìœ ë„í•˜ê¸° ìœ„í•´ ë¹„ì •ìƒì ìœ¼ë¡œ êµ¬ì„±ë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë¹„ì •ìƒì ì¸ ê¸¸ì´, ë°˜ë³µë˜ëŠ” ë¬¸ìì—´, ì¸ì½”ë”©ëœ ë¬¸ìì—´ ë“±ì´ ì—¬ê¸°ì— í•´ë‹¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                display_text = f"URL ë‚´ì˜ '{word}' íŒ¨í„´ì´ {predicted_label_context_for_meaning} {('ì¦ê°€ì‹œí‚¤ëŠ”' if weight > 0 else 'ê°ì†Œì‹œí‚¤ëŠ”')} ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤."
            
            return display_text, meaning

        # --- ìš”ì•½ ì„¤ëª… ìƒì„± (reason_summary) ---
        significant_features_for_summary = []
        for word, weight in explanation_list:
            if abs(weight) > 0.05: # ì¢€ ë” ë†’ì€ ì„ê³„ê°’ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ìš” íŠ¹ì§•ë§Œ ìš”ì•½ì— í¬í•¨
                # ìš”ì•½ì—ëŠ” í‚¤ì›Œë“œ ìì²´ë§Œ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨ (LIMEì´ ë¶„ì„í•œ ë‹¨ì–´ ê·¸ëŒ€ë¡œ)
                # desc_textê°€ Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€
                desc_text, _ = get_understandable_explanation_text_and_meaning(word, weight, "") # ì‹¤ì œ ì„¤ëª… ìƒì„±ì€ ì•„ë‹ˆê³ , í•„í„°ë§ ëª©ì ìœ¼ë¡œ í˜¸ì¶œ
                if desc_text: # í•„í„°ë§ëœ ë‹¨ì–´ ì¤‘ ìœ íš¨í•œ ê²ƒë§Œ ì¶”ê°€
                    
                    significant_features_for_summary.append(word)

        # ìš”ì•½ ë¬¸êµ¬ì— HTTP í—¤ë” ì •ë³´ ë¶€ì¬ê°€ ì¤‘ìš”í•œ ì•…ì„± ì‹ í˜¸ì¼ ê²½ìš° í¬í•¨
        if predicted_label == 'malicious':
            if 'NOHEADER' in [f[0].lower() for f in explanation_list if abs(f[1]) > 0.05]: # NOHEADERê°€ ì£¼ìš” íŠ¹ì§•ìœ¼ë¡œ ê¼½í˜”ì„ ë•Œ
                reason_phrases.append(f"íŠ¹íˆ **HTTP í—¤ë” ì •ë³´ ë¶€ì¬**ì™€ '{', '.join(significant_features_for_summary[:2])}' ë“±ê³¼ ê°™ì´ **ì •ìƒì ì´ì§€ ì•Šì€ íŒ¨í„´**ì´ ì•…ì„±ìœ¼ë¡œ ì˜ì‹¬ë©ë‹ˆë‹¤.")
            elif significant_features_for_summary:
                reason_phrases.append(f"íŠ¹íˆ '{', '.join(significant_features_for_summary[:3])}' ë“±ê³¼ ê°™ì´ **ì •ìƒì ì´ì§€ ì•Šì€ íŒ¨í„´**ì´ ì•…ì„±ìœ¼ë¡œ ì˜ì‹¬ë©ë‹ˆë‹¤.")
            else:
                reason_phrases.append("URLì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ì™€ íŒ¨í„´ì´ ì•Œë ¤ì§„ ì•…ì„± URLê³¼ ìœ ì‚¬í•˜ì—¬ ì˜ì‹¬ë©ë‹ˆë‹¤.")
        else: # 'benign'
            if significant_features_for_summary:
                reason_phrases.append(f"ì´ URLì€ '{', '.join(significant_features_for_summary[:3])}' ë“±ê³¼ ê°™ì´ **ì¼ë°˜ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŒ¨í„´**ì„ ê°€ì§€ê³  ìˆì–´ ì•ˆì „í•©ë‹ˆë‹¤.")
            else:
                reason_phrases.append("URLì˜ ì „ë°˜ì ì¸ êµ¬ì¡°ì™€ íŒ¨í„´ì´ ì•Œë ¤ì§„ ì •ìƒ URLê³¼ ìœ ì‚¬í•˜ì—¬ ì•ˆì „í•©ë‹ˆë‹¤.")

        reason_summary = "ì´ URLì€ " + predicted_label.upper() + "ë¡œ íŒë‹¨ë©ë‹ˆë‹¤. " + " ".join(reason_phrases)

        # --- ìƒì„¸ ì„¤ëª… ì¶œë ¥ ---
        print("\n--- ìƒì„¸ ë¶„ì„ (URL íŠ¹ì§•ë³„ ê¸°ì—¬ë„) ---")
        print("ğŸ’¡ ì´ ì„¹ì…˜ì—ì„œëŠ” ëª¨ë¸ì´ URLì„ ë¶„ì„í•˜ë©° ì¤‘ìš”í•˜ê²Œ íŒë‹¨í•œ ì£¼ìš” íŠ¹ì§•ë“¤ê³¼ ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.")
        print("   'ì˜í–¥ë„'ëŠ” ê° íŠ¹ì§•ì´ ìµœì¢… íŒë‹¨ì— ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ê¸°ì—¬í–ˆëŠ”ì§€ë¥¼ ìˆ«ìë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. (ê°’ì´ í´ìˆ˜ë¡ ê¸°ì—¬ë„ ë†’ìŒ)\n")
        
        has_understandable_explanation = False
        for word, weight in explanation.as_list(label=predicted_class_id): 
            classification_impact_for_meaning = ""
            if predicted_label == 'malicious':
                classification_impact_for_meaning = "ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë  ê°€ëŠ¥ì„±ì„" if weight > 0 else "ì •ìƒìœ¼ë¡œ íŒë‹¨ë  ê°€ëŠ¥ì„±ì„"
            else: # benign
                classification_impact_for_meaning = "ì •ìƒìœ¼ë¡œ íŒë‹¨ë  ê°€ëŠ¥ì„±ì„" if weight > 0 else "ì•…ì„±ìœ¼ë¡œ íŒë‹¨ë  ê°€ëŠ¥ì„±ì„"

            desc_text, meaning = get_understandable_explanation_text_and_meaning(word, weight, classification_impact_for_meaning)
            
            if desc_text and abs(weight) > 0.01: # ì˜í–¥ë„ê°€ 0.01 ì´ìƒì¸ ê²½ìš°ë§Œ ì¶œë ¥
                print(f"  - **íŠ¹ì§•**: {desc_text} (ì˜í–¥ë„: {weight:.4f})")
                if meaning:
                    print(f"    **ì„¤ëª…**: {meaning}\n")
                has_understandable_explanation = True
        
        if not has_understandable_explanation:
            print("ìë™ í•„í„°ë§ëœ ì£¼ìš” íŠ¹ì§•ì€ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ LIME ê²°ê³¼ì—ëŠ” ë³µì¡í•œ ë¬¸ìì—´ì´ë‚˜ ë¯¸ë¯¸í•œ ì˜í–¥ì´ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("----------------------------")
        print(" ì´ ë¶„ì„ì€ ëª¨ë¸ì´ í•™ìŠµí•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•˜ë©°, ëª¨ë“  URLì— ëŒ€í•œ ì ˆëŒ€ì ì¸ íŒë‹¨ ê¸°ì¤€ì€ ì•„ë‹™ë‹ˆë‹¤. ")
        print("   ì˜ì‹¬ìŠ¤ëŸ¬ìš´ URLì€ ì§ì ‘ ì ‘ì†í•˜ê¸° ì „ ë°˜ë“œì‹œ ì£¼ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n")


    except Exception as e:
        print(f"LIME ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        reason_summary = f"ì´ URLì€ {predicted_label.upper()}ë¡œ íŒë‹¨ë©ë‹ˆë‹¤ (í™•ì‹ ë„: {confidence:.2f}%). íŒë‹¨ ê·¼ê±°ë¥¼ ì„¤ëª…í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        explanation_list = []

    print(f"ë¶„ë¥˜ ê²°ê³¼: **{predicted_label.upper()}** (í™•ì‹ ë„: {confidence:.2f}%)")
    print(f"ì„¤ëª… ìš”ì•½: {reason_summary}")

    return {
        "predicted_label": predicted_label,
        "confidence": f"{confidence:.2f}%",
        "reason_summary": reason_summary,
        "detailed_explanation": explanation_list # ì›ë³¸ LIME ê²°ê³¼ë¥¼ ë°˜í™˜
    }